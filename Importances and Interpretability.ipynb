{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING RANDOM FORESTS\n",
    "\n",
    "30/01/2020 - Davide di Nello - ECB S2S Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import  and prepare Titanic dataset\n",
    "\n",
    "Data can be fetched via openml, or downloaded manually from https://www.openml.org/d/40945 <br>\n",
    "When importing data manually, we first need to clean some of it to be sure that values are interpreted correctly <br>\n",
    "Also, we will generate two random features, in order to see their effect on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True) # gives proxy problems\n",
    "df = pd.read_csv('phpMYEkMl.csv')  # Manual import of the file\n",
    "\n",
    "y = df.survived \n",
    "X = df.drop('survived', axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X == '?'] = np.nan  # Missing values are represented by a question mark - replace them with NaN\n",
    "X.age = X.age.astype(float)  # Some numeric variables were interpreted as strings because of missing values. \n",
    "X.fare = X.fare.astype(float) # Transform them back to numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection:** <br>\n",
    "We remove useless predictors: name, ticket and destination are by definition sparse and bring no information. <br> Boat and body, on the other hand, are perfectly correlated on our target variable \"survived\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate two random predictors, to see their effect on feature importances\n",
    "rng = np.random.RandomState(seed=42)\n",
    "X['random_cat'] = rng.randint(3, size=X.shape[0])\n",
    "X['random_num'] = rng.randn(X.shape[0])\n",
    "\n",
    "# Separate categorical and numerical predictors for preprocessing\n",
    "categorical_columns = ['pclass', 'sex', 'embarked', 'random_cat']\n",
    "numerical_columns = ['age', 'sibsp', 'parch', 'fare', 'random_num']\n",
    "\n",
    "X = X[categorical_columns + numerical_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preparation:** <br>\n",
    "For categorical variables: Impute missing as a new category and encode numerically <br>\n",
    "For numerical variables: Impute missing with the mean<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[categorical_columns] = X[categorical_columns].fillna('missing') \n",
    "oe = OrdinalEncoder().fit(X[categorical_columns])\n",
    "X[categorical_columns] = oe.transform(X[categorical_columns])\n",
    "\n",
    "for elem in numerical_columns:\n",
    "    X[elem] = X[elem].fillna(X[elem].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train and test data, stratifying over the target class\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn's RandomForestClassifier\n",
    "Let's try first to use it with default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"RF train accuracy: %0.3f\" % rf.score(X_train, y_train))\n",
    "print(\"RF test accuracy: %0.3f\" % rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters Optimization:** <br>\n",
    "Let's take advantage of the Out Of Bag error of Random Forest to estimate the best parameters. <br>\n",
    "We can try multiple values of `max_features` and `max_depth` and at the same time check the progress of the OOB score as we grow more and more trees. <p>\n",
    "We will check OOB error while growing from 10 to 500 trees in the Random Forest, testing the algorithm on a discrete grid over `max_features: [sqrt(n), n/2, n]` and `max_depth: [5, 10, None]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feat = [\"sqrt\", 0.5, None]\n",
    "depth = [5, 10, None]\n",
    "\n",
    "ensemble_clfs = [(\"RF, max_features={}, max_depth={}\".format(x,y),\n",
    "        RandomForestClassifier(warm_start=True, oob_score=True,\n",
    "                               max_features=x,\n",
    "                               random_state=42,\n",
    "                               max_depth=y)) for x in max_feat for y in depth]\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 10\n",
    "max_estimators = 500\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in tqdm(range(min_estimators, max_estimators + 1, 10)):\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot these OOB errors:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the best option is to use `max_features=None`, and its OOB error looks stable from around `n_estimators=200`. <br>\n",
    "Therefore, let's use these parameters for our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, max_features='sqrt', n_estimators=200, max_depth=10)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"RF train accuracy: %0.3f\" % rf.score(X_train, y_train))\n",
    "print(\"RF test accuracy: %0.3f\" % rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability: Variable Importances and Dependency plots\n",
    "Every implementation of Random Forest allows access to Variable Importances. <br>\n",
    "These are computed by summing all the impurity decreases that were encountered when doing a split on a given variable.<br>\n",
    "Let's see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feature_importances = (\n",
    "    rf.feature_importances_)\n",
    "sorted_idx = tree_feature_importances.argsort()\n",
    "\n",
    "y_ticks = np.arange(0, len(X.columns))\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(y_ticks, tree_feature_importances[sorted_idx])\n",
    "ax.set_yticklabels(X.columns[sorted_idx])\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is it possible that the importance of the random numerical predictor is so high?\n",
    "<p>\n",
    "**Permutation Importance:** <br>\n",
    "Instead of looking at the impurity when building the trees, let's see how the performance changes over the test set when shuffling a variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(rf, X_test, y_test, n_repeats=10,\n",
    "                                random_state=1, n_jobs=-1)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_test.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is however important to only trust your variable importances if your model is strong enough. If your model is weak and generalizes badly, importances have usually little meaning and they might fluctuate importantly from one run to another.\n",
    "\n",
    "We now know which features are the most important. <br>\n",
    "But can we also know do they affect the chance of survival? <p>\n",
    "**Partial Dependency Plots:** <br>\n",
    "These plots marginalize the contributions of the single variables and give us an idea of how each variable impacts the output. <br>\n",
    "Bear in mind, however, that these relationships are just descriptive and assume independence between the plotted variable and the others, so handle with care!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = ['age', 'pclass', 'sex']\n",
    "plot_partial_dependence(rf, X, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When checking the inverse transformation of our classes, we see that values [0,1,2] for pclass correspond to 1st, 2nd and 3rd class, while for sex, 0 represents female and 1 represents males. <p>\n",
    "**All together, if you were on the Titanic, you'd have had the highest chance of survival by being a baby girl in 1st class!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extratrees, Calibration, Encoding, Gini"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
